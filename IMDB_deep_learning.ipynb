{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.metrics import Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing Imdb dataset into train data and test data\n",
    "(train_data, train_labels), (test_data,test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the input data is in integers,we have to chage that to tensors of 1's and 0's.\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation = 'relu', input_shape = (input_dimension,)),\n",
    "    layers.Dense(16, activation = 'relu'),\n",
    "    layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the optimizer, loss function and metric\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = [Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The train data is divided into validation set to guide the training process.\n",
    "x_val = x_train[:10000]\n",
    "p_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "p_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 2s 48ms/step - loss: 0.5367 - precision: 0.7933 - val_loss: 0.4177 - val_precision: 0.8836\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.3407 - precision: 0.8865 - val_loss: 0.3239 - val_precision: 0.8596\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.2543 - precision: 0.9099 - val_loss: 0.2876 - val_precision: 0.8787\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.2069 - precision: 0.9257 - val_loss: 0.2973 - val_precision: 0.9251\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.1723 - precision: 0.9406 - val_loss: 0.3021 - val_precision: 0.8355\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.1500 - precision: 0.9497 - val_loss: 0.3132 - val_precision: 0.9263\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.1290 - precision: 0.9612 - val_loss: 0.3213 - val_precision: 0.9216\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.1140 - precision: 0.9657 - val_loss: 0.3176 - val_precision: 0.8502\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0974 - precision: 0.9713 - val_loss: 0.3156 - val_precision: 0.8884\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0875 - precision: 0.9747 - val_loss: 0.3313 - val_precision: 0.8895\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(p_x_train,\n",
    "                    p_y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3580 - precision: 0.8860\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3579956293106079, 0.8859795928001404]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "30/30 [==============================] - 2s 53ms/step - loss: 0.0822 - acc: 0.9755 - val_loss: 0.3518 - val_acc: 0.8737\n",
      "Epoch 2/4\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.0635 - acc: 0.9857 - val_loss: 0.3664 - val_acc: 0.8798\n",
      "Epoch 3/4\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.0562 - acc: 0.9864 - val_loss: 0.3982 - val_acc: 0.8770\n",
      "Epoch 4/4\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.0486 - acc: 0.9883 - val_loss: 0.4011 - val_acc: 0.8765\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4309 - acc: 0.8655\n",
      "[0.4309108853340149, 0.8655200004577637]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['acc']) #Change in metric\n",
    "\n",
    "history = model.fit(p_x_train,\n",
    "                    p_y_train,\n",
    "                    epochs = 4,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val,y_val))\n",
    "\n",
    "results = model.evaluate(x_test,y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "30/30 [==============================] - 3s 59ms/step - loss: 0.5471 - acc: 0.7839 - val_loss: 0.4211 - val_acc: 0.8583\n",
      "Epoch 2/4\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.3385 - acc: 0.8884 - val_loss: 0.3290 - val_acc: 0.8700\n",
      "Epoch 3/4\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.2468 - acc: 0.9137 - val_loss: 0.2960 - val_acc: 0.8813\n",
      "Epoch 4/4\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.2012 - acc: 0.9289 - val_loss: 0.2915 - val_acc: 0.8832\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3027 - acc: 0.8767\n",
      "[0.3027161955833435, 0.8767200112342834]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation = 'relu', input_shape = (input_dimension,)),\n",
    "    layers.Dense(16, activation = 'relu'),\n",
    "    layers.Dense(16, activation = 'relu'),# Added a thrid layer to check it's effect\n",
    "    layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['acc']) \n",
    "\n",
    "history = model.fit(p_x_train,\n",
    "                    p_y_train,\n",
    "                    epochs = 4,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val,y_val))\n",
    "\n",
    "results = model.evaluate(x_test,y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "30/30 [==============================] - 3s 59ms/step - loss: 0.6052 - acc: 0.6577 - val_loss: 0.5170 - val_acc: 0.8320\n",
      "Epoch 2/4\n",
      "30/30 [==============================] - 1s 20ms/step - loss: 0.4480 - acc: 0.8708 - val_loss: 0.4490 - val_acc: 0.8487\n",
      "Epoch 3/4\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.3058 - acc: 0.9158 - val_loss: 0.3186 - val_acc: 0.8826\n",
      "Epoch 4/4\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.2154 - acc: 0.9334 - val_loss: 0.3003 - val_acc: 0.8826\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3112 - acc: 0.8766\n",
      "[0.31116050481796265, 0.8765599727630615]\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation = 'relu', input_shape = (input_dimension,)),\n",
    "    layers.Dense(16, activation = 'relu'),\n",
    "    layers.Dense(16, activation = 'relu'),# Added a thrid layer to check it's effect\n",
    "    layers.Dense(16, activation = 'relu'),# Added a fourth layer\n",
    "    layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['acc']) \n",
    "\n",
    "history = model.fit(p_x_train,\n",
    "                    p_y_train,\n",
    "                    epochs = 4,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val,y_val))\n",
    "\n",
    "results = model.evaluate(x_test,y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "30/30 [==============================] - 3s 59ms/step - loss: 0.5055 - acc: 0.7539 - val_loss: 0.4506 - val_acc: 0.7893\n",
      "Epoch 2/4\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2973 - acc: 0.8871 - val_loss: 0.2946 - val_acc: 0.8859\n",
      "Epoch 3/4\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2185 - acc: 0.9195 - val_loss: 0.2781 - val_acc: 0.8870\n",
      "Epoch 4/4\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1806 - acc: 0.9352 - val_loss: 0.3249 - val_acc: 0.8714\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3467 - acc: 0.8587\n",
      "[0.34669992327690125, 0.858680009841919]\n"
     ]
    }
   ],
   "source": [
    "#Changed the hidden unit dimensions\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation = 'relu', input_shape = (input_dimension,)),\n",
    "    layers.Dense(32, activation = 'relu'),\n",
    "    layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['acc']) \n",
    "\n",
    "history = model.fit(p_x_train,\n",
    "                    p_y_train,\n",
    "                    epochs = 4,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val,y_val))\n",
    "\n",
    "results = model.evaluate(x_test,y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "30/30 [==============================] - 3s 76ms/step - loss: 0.5536 - mse: 0.1848 - val_loss: 0.4250 - val_mse: 0.1292\n",
      "Epoch 2/4\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.3526 - mse: 0.1027 - val_loss: 0.3291 - val_mse: 0.0969\n",
      "Epoch 3/4\n",
      "30/30 [==============================] - 5s 156ms/step - loss: 0.2623 - mse: 0.0734 - val_loss: 0.2940 - val_mse: 0.0871\n",
      "Epoch 4/4\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.2119 - mse: 0.0585 - val_loss: 0.2821 - val_mse: 0.0840\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2941 - mse: 0.0879\n",
      "[0.2941144108772278, 0.08792287856340408]\n"
     ]
    }
   ],
   "source": [
    "#Changed the hidden unit dimensions\n",
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation = 'relu', input_shape = (input_dimension,)),\n",
    "    layers.Dense(16, activation = 'relu'),\n",
    "    layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['mse']) \n",
    "\n",
    "history = model.fit(p_x_train,\n",
    "                    p_y_train,\n",
    "                    epochs = 4,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val,y_val))\n",
    "\n",
    "results = model.evaluate(x_test,y_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function change from Binary crossentropy to MSE has decreased the accuracy from 85% to 0.08%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
